{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_DHZn5aS3Ib",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "3690482e-81f9-4659-87df-5773d58804c2"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import csv\n",
        "\n",
        "timesteps = 3\n",
        "\n",
        "num_classes = timesteps  # follow timestep\n",
        "data_dim = 15\n",
        "batchsize = 32 # number of data in a batch\n",
        "drop = 0.2\n",
        "\n",
        "#%%\n",
        "# load dataset\n",
        "dataset = pd.read_csv(\"/content/Index Time Series Multi Variate (Edited).csv\")\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "data_size = len(dataset) # datasize selected must have both attack and normal data.\n",
        "data_resize = int(data_size//timesteps) #data_size/timesteps using // because round down, example 10/3=3\n",
        "data_trunc_size = data_resize * timesteps # remove extra rows for so that data can be divided by timesteps\n",
        "\n",
        "df = df[:data_trunc_size]\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close_10year_treasury</th>\n",
              "      <th>Volume_10year_treasury</th>\n",
              "      <th>Close_copper</th>\n",
              "      <th>Volume_copper</th>\n",
              "      <th>Close_gold</th>\n",
              "      <th>Volume_gold</th>\n",
              "      <th>Close_hk_index</th>\n",
              "      <th>Volume_hk_index</th>\n",
              "      <th>Close_oil</th>\n",
              "      <th>Volume_oil</th>\n",
              "      <th>Close_s&amp;p</th>\n",
              "      <th>Volume_s&amp;p</th>\n",
              "      <th>Value_us_sgd</th>\n",
              "      <th>Increase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3/27/2015</td>\n",
              "      <td>1.948</td>\n",
              "      <td>0</td>\n",
              "      <td>2.7750</td>\n",
              "      <td>483</td>\n",
              "      <td>1199.800049</td>\n",
              "      <td>68</td>\n",
              "      <td>24486.19922</td>\n",
              "      <td>1610426400</td>\n",
              "      <td>48.869999</td>\n",
              "      <td>515277</td>\n",
              "      <td>2061.020020</td>\n",
              "      <td>3008550000</td>\n",
              "      <td>1.3690</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3/30/2015</td>\n",
              "      <td>1.963</td>\n",
              "      <td>0</td>\n",
              "      <td>2.7875</td>\n",
              "      <td>714</td>\n",
              "      <td>1184.800049</td>\n",
              "      <td>172260</td>\n",
              "      <td>24855.11914</td>\n",
              "      <td>2574119600</td>\n",
              "      <td>48.680000</td>\n",
              "      <td>384092</td>\n",
              "      <td>2086.239990</td>\n",
              "      <td>2917690000</td>\n",
              "      <td>1.3759</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3/31/2015</td>\n",
              "      <td>1.934</td>\n",
              "      <td>0</td>\n",
              "      <td>2.7470</td>\n",
              "      <td>671</td>\n",
              "      <td>1183.099976</td>\n",
              "      <td>37990</td>\n",
              "      <td>24900.89063</td>\n",
              "      <td>2557002200</td>\n",
              "      <td>47.599998</td>\n",
              "      <td>355866</td>\n",
              "      <td>2067.889893</td>\n",
              "      <td>3376550000</td>\n",
              "      <td>1.3716</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4/1/2015</td>\n",
              "      <td>1.868</td>\n",
              "      <td>0</td>\n",
              "      <td>2.7565</td>\n",
              "      <td>401</td>\n",
              "      <td>1208.099976</td>\n",
              "      <td>3409</td>\n",
              "      <td>25082.75000</td>\n",
              "      <td>1713463600</td>\n",
              "      <td>50.090000</td>\n",
              "      <td>365748</td>\n",
              "      <td>2059.689941</td>\n",
              "      <td>3543270000</td>\n",
              "      <td>1.3622</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4/2/2015</td>\n",
              "      <td>1.904</td>\n",
              "      <td>0</td>\n",
              "      <td>2.7480</td>\n",
              "      <td>550</td>\n",
              "      <td>1200.900024</td>\n",
              "      <td>1632</td>\n",
              "      <td>25275.64063</td>\n",
              "      <td>2106782000</td>\n",
              "      <td>49.139999</td>\n",
              "      <td>451104</td>\n",
              "      <td>2066.959961</td>\n",
              "      <td>3095960000</td>\n",
              "      <td>1.3567</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Date  Close_10year_treasury  ...  Value_us_sgd  Increase\n",
              "0  3/27/2015                  1.948  ...        1.3690      True\n",
              "1  3/30/2015                  1.963  ...        1.3759      True\n",
              "2  3/31/2015                  1.934  ...        1.3716     False\n",
              "3   4/1/2015                  1.868  ...        1.3622      True\n",
              "4   4/2/2015                  1.904  ...        1.3567      True\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADG0selAhiX8",
        "outputId": "79996f0e-fd3c-4c2b-eaa3-023c255106cb"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1158, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "irICQPZYhO2L",
        "outputId": "7f84f975-ffe9-4121-9841-65a360c9fd13"
      },
      "source": [
        "df.head().T"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <td>3/27/2015</td>\n",
              "      <td>3/30/2015</td>\n",
              "      <td>3/31/2015</td>\n",
              "      <td>4/1/2015</td>\n",
              "      <td>4/2/2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Close_10year_treasury</th>\n",
              "      <td>1.948</td>\n",
              "      <td>1.963</td>\n",
              "      <td>1.934</td>\n",
              "      <td>1.868</td>\n",
              "      <td>1.904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Volume_10year_treasury</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Close_copper</th>\n",
              "      <td>2.775</td>\n",
              "      <td>2.7875</td>\n",
              "      <td>2.747</td>\n",
              "      <td>2.7565</td>\n",
              "      <td>2.748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Volume_copper</th>\n",
              "      <td>483</td>\n",
              "      <td>714</td>\n",
              "      <td>671</td>\n",
              "      <td>401</td>\n",
              "      <td>550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Close_gold</th>\n",
              "      <td>1199.8</td>\n",
              "      <td>1184.8</td>\n",
              "      <td>1183.1</td>\n",
              "      <td>1208.1</td>\n",
              "      <td>1200.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Volume_gold</th>\n",
              "      <td>68</td>\n",
              "      <td>172260</td>\n",
              "      <td>37990</td>\n",
              "      <td>3409</td>\n",
              "      <td>1632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Close_hk_index</th>\n",
              "      <td>24486.2</td>\n",
              "      <td>24855.1</td>\n",
              "      <td>24900.9</td>\n",
              "      <td>25082.8</td>\n",
              "      <td>25275.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Volume_hk_index</th>\n",
              "      <td>1610426400</td>\n",
              "      <td>2574119600</td>\n",
              "      <td>2557002200</td>\n",
              "      <td>1713463600</td>\n",
              "      <td>2106782000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Close_oil</th>\n",
              "      <td>48.87</td>\n",
              "      <td>48.68</td>\n",
              "      <td>47.6</td>\n",
              "      <td>50.09</td>\n",
              "      <td>49.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Volume_oil</th>\n",
              "      <td>515277</td>\n",
              "      <td>384092</td>\n",
              "      <td>355866</td>\n",
              "      <td>365748</td>\n",
              "      <td>451104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Close_s&amp;p</th>\n",
              "      <td>2061.02</td>\n",
              "      <td>2086.24</td>\n",
              "      <td>2067.89</td>\n",
              "      <td>2059.69</td>\n",
              "      <td>2066.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Volume_s&amp;p</th>\n",
              "      <td>3008550000</td>\n",
              "      <td>2917690000</td>\n",
              "      <td>3376550000</td>\n",
              "      <td>3543270000</td>\n",
              "      <td>3095960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Value_us_sgd</th>\n",
              "      <td>1.369</td>\n",
              "      <td>1.3759</td>\n",
              "      <td>1.3716</td>\n",
              "      <td>1.3622</td>\n",
              "      <td>1.3567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Increase</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 0           1  ...           3           4\n",
              "Date                     3/27/2015   3/30/2015  ...    4/1/2015    4/2/2015\n",
              "Close_10year_treasury        1.948       1.963  ...       1.868       1.904\n",
              "Volume_10year_treasury           0           0  ...           0           0\n",
              "Close_copper                 2.775      2.7875  ...      2.7565       2.748\n",
              "Volume_copper                  483         714  ...         401         550\n",
              "Close_gold                  1199.8      1184.8  ...      1208.1      1200.9\n",
              "Volume_gold                     68      172260  ...        3409        1632\n",
              "Close_hk_index             24486.2     24855.1  ...     25082.8     25275.6\n",
              "Volume_hk_index         1610426400  2574119600  ...  1713463600  2106782000\n",
              "Close_oil                    48.87       48.68  ...       50.09       49.14\n",
              "Volume_oil                  515277      384092  ...      365748      451104\n",
              "Close_s&p                  2061.02     2086.24  ...     2059.69     2066.96\n",
              "Volume_s&p              3008550000  2917690000  ...  3543270000  3095960000\n",
              "Value_us_sgd                 1.369      1.3759  ...      1.3622      1.3567\n",
              "Increase                      True        True  ...        True        True\n",
              "\n",
              "[15 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV8RrGO8g8gA",
        "outputId": "3663d4b6-a585-40e6-862f-5c822489dfe9"
      },
      "source": [
        "\n",
        "# Remove the dates, therefore X start from 1\n",
        "X = df.iloc[:,1:-1].values\n",
        "Y = df.iloc[:,-1].values\n",
        "\n",
        "print(X[:5])\n",
        "print(Y[:5])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.94800000e+00 0.00000000e+00 2.77500000e+00 4.83000000e+02\n",
            "  1.19980005e+03 6.80000000e+01 2.44861992e+04 1.61042640e+09\n",
            "  4.88699990e+01 5.15277000e+05 2.06102002e+03 3.00855000e+09\n",
            "  1.36900000e+00]\n",
            " [1.96300000e+00 0.00000000e+00 2.78750000e+00 7.14000000e+02\n",
            "  1.18480005e+03 1.72260000e+05 2.48551191e+04 2.57411960e+09\n",
            "  4.86800000e+01 3.84092000e+05 2.08623999e+03 2.91769000e+09\n",
            "  1.37590000e+00]\n",
            " [1.93400000e+00 0.00000000e+00 2.74700000e+00 6.71000000e+02\n",
            "  1.18309998e+03 3.79900000e+04 2.49008906e+04 2.55700220e+09\n",
            "  4.75999980e+01 3.55866000e+05 2.06788989e+03 3.37655000e+09\n",
            "  1.37160000e+00]\n",
            " [1.86800000e+00 0.00000000e+00 2.75650000e+00 4.01000000e+02\n",
            "  1.20809998e+03 3.40900000e+03 2.50827500e+04 1.71346360e+09\n",
            "  5.00900000e+01 3.65748000e+05 2.05968994e+03 3.54327000e+09\n",
            "  1.36220000e+00]\n",
            " [1.90400000e+00 0.00000000e+00 2.74800000e+00 5.50000000e+02\n",
            "  1.20090002e+03 1.63200000e+03 2.52756406e+04 2.10678200e+09\n",
            "  4.91399990e+01 4.51104000e+05 2.06695996e+03 3.09596000e+09\n",
            "  1.35670000e+00]]\n",
            "[ True  True False  True  True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6UvN6mqjpJ7",
        "outputId": "8cd4369c-94cc-4295-f90e-10a3a6e69df5"
      },
      "source": [
        "# Reduce the size of data so that the data can be divided by time step\n",
        "# split into input (X) and output (Y) variables\n",
        "\n",
        "\n",
        "# required format for lstm\n",
        "X_shaped = X.reshape(data_resize, timesteps, data_dim-2).astype('float32')\n",
        "Y_shaped = Y.reshape(data_resize,timesteps).astype('bool')\n",
        "print(\"X shape is : {}\".format(X_shaped.shape))\n",
        "print(\"Y shape is : {}\".format(Y_shaped.shape))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape is : (386, 3, 13)\n",
            "Y shape is : (386, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkQBgMcEjxuo"
      },
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_shaped, Y_shaped, test_size=0.3)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k68DTLWj0cp"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaVX3d3tjhZu",
        "outputId": "4cce70e8-7150-47d6-8872-46714c60f474"
      },
      "source": [
        "\n",
        "\n",
        "# Create the model\n",
        "# expected input data shape: (batch_size, timesteps, data_dim)\n",
        "# Dropout used to prevent over-fitting.\n",
        "# Input shape will infer the batch size by itself\n",
        "# We are using binary cross entropy even when num_class can be > 1 because this is a binary classification on an array\n",
        "# For multiclass classification, use categorical cross-entropy\n",
        "model = Sequential()\n",
        "model.add(LSTM(36, return_sequences=True, input_shape=(timesteps, data_dim-2)))  # returns a sequence of vectors of dimension 40\n",
        "model.add(Dropout(drop))\n",
        "model.add(LSTM(36,return_sequences=True))  # returns a sequence of vectors of dimension 40\n",
        "model.add(Dropout(drop))\n",
        "model.add(LSTM(36,return_sequences=True))  # returns a sequence of vectors of dimension 40\n",
        "model.add(Dropout(drop))\n",
        "model.add(LSTM(36))  # return a single vector of dimension 40\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(num_classes, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size= batchsize, epochs=100, validation_data= (X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "# Returns you the accuracy and loss\n",
        "loss, acc = model.evaluate(X_train, y_train,timesteps)\n",
        "print(\"Keras: \\n%s: %.2f%%\" % (model.metrics_names[1], acc*100))\n",
        "\n",
        "# Shape of prediction is nrow * timestep\n",
        "# Result would be that same as keras evaluate\n",
        "proba = model.predict(X_test)\n",
        "\n",
        "# proba is the probability. Here we set threshold as 0.5 to be considered true.\n",
        "print('predictions shape:', proba.shape)\n",
        "y_pred = proba > 0.5\n",
        "# Reshape to a single dimension for comparison and to create confusion matrix\n",
        "y_pred_single_dim = y_pred.reshape(proba.shape[0]*proba.shape[1])\n",
        "y_test_single_dim = y_test.reshape(y_test.shape[0]*y_test.shape[1])\n",
        "print(y_pred_single_dim)\n",
        "print(y_test_single_dim)\n",
        "matrix = confusion_matrix(y_test_single_dim, y_pred_single_dim)\n",
        "print(\"Keras: \\n%s: %.2f%%\" % (model.metrics_names[1], sum(y_pred_single_dim==y_test_single_dim)/len(y_test_single_dim)*100))\n",
        "print(matrix)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "9/9 [==============================] - 1s 144ms/step - loss: 0.6937 - accuracy: 0.4556 - val_loss: 0.6921 - val_accuracy: 0.6724\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6930 - accuracy: 0.6444 - val_loss: 0.6919 - val_accuracy: 0.6724\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6929 - accuracy: 0.6444 - val_loss: 0.6920 - val_accuracy: 0.6724\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.6444 - val_loss: 0.6919 - val_accuracy: 0.6724\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6932 - accuracy: 0.6444 - val_loss: 0.6917 - val_accuracy: 0.6724\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6926 - accuracy: 0.6444 - val_loss: 0.6913 - val_accuracy: 0.6724\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.6444 - val_loss: 0.6911 - val_accuracy: 0.6724\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6925 - accuracy: 0.6444 - val_loss: 0.6914 - val_accuracy: 0.6724\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6929 - accuracy: 0.6444 - val_loss: 0.6911 - val_accuracy: 0.6724\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6927 - accuracy: 0.6407 - val_loss: 0.6916 - val_accuracy: 0.6724\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6922 - accuracy: 0.6481 - val_loss: 0.6914 - val_accuracy: 0.6638\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6922 - accuracy: 0.6370 - val_loss: 0.6910 - val_accuracy: 0.6724\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6929 - accuracy: 0.6444 - val_loss: 0.6908 - val_accuracy: 0.6724\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.6927 - accuracy: 0.6370 - val_loss: 0.6909 - val_accuracy: 0.6724\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6919 - accuracy: 0.6444 - val_loss: 0.6911 - val_accuracy: 0.6552\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6925 - accuracy: 0.6370 - val_loss: 0.6913 - val_accuracy: 0.6552\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6917 - accuracy: 0.6407 - val_loss: 0.6912 - val_accuracy: 0.6466\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.6916 - accuracy: 0.6444 - val_loss: 0.6915 - val_accuracy: 0.6293\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.6917 - accuracy: 0.6148 - val_loss: 0.6915 - val_accuracy: 0.6293\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.6929 - accuracy: 0.6148 - val_loss: 0.6919 - val_accuracy: 0.5690\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.6914 - accuracy: 0.5630 - val_loss: 0.6914 - val_accuracy: 0.6293\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.6927 - accuracy: 0.6037 - val_loss: 0.6913 - val_accuracy: 0.6293\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.6918 - accuracy: 0.5778 - val_loss: 0.6909 - val_accuracy: 0.6466\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6916 - accuracy: 0.6185 - val_loss: 0.6911 - val_accuracy: 0.6293\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6920 - accuracy: 0.5704 - val_loss: 0.6915 - val_accuracy: 0.5603\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.6906 - accuracy: 0.5667 - val_loss: 0.6914 - val_accuracy: 0.5172\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.6915 - accuracy: 0.5667 - val_loss: 0.6916 - val_accuracy: 0.5172\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.6923 - accuracy: 0.5259 - val_loss: 0.6916 - val_accuracy: 0.5086\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.6918 - accuracy: 0.5000 - val_loss: 0.6916 - val_accuracy: 0.5086\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6919 - accuracy: 0.5148 - val_loss: 0.6913 - val_accuracy: 0.5172\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6907 - accuracy: 0.5519 - val_loss: 0.6910 - val_accuracy: 0.5259\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6890 - accuracy: 0.5704 - val_loss: 0.6912 - val_accuracy: 0.5086\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6911 - accuracy: 0.5741 - val_loss: 0.6908 - val_accuracy: 0.5172\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6920 - accuracy: 0.4963 - val_loss: 0.6912 - val_accuracy: 0.4569\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6914 - accuracy: 0.4741 - val_loss: 0.6905 - val_accuracy: 0.5259\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6933 - accuracy: 0.5407 - val_loss: 0.6902 - val_accuracy: 0.6293\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6913 - accuracy: 0.5037 - val_loss: 0.6898 - val_accuracy: 0.6379\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.5815 - val_loss: 0.6901 - val_accuracy: 0.6121\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6917 - accuracy: 0.5407 - val_loss: 0.6897 - val_accuracy: 0.6466\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6885 - accuracy: 0.6148 - val_loss: 0.6899 - val_accuracy: 0.6379\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.6910 - accuracy: 0.5778 - val_loss: 0.6903 - val_accuracy: 0.5345\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6890 - accuracy: 0.5593 - val_loss: 0.6902 - val_accuracy: 0.5172\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6931 - accuracy: 0.5333 - val_loss: 0.6900 - val_accuracy: 0.6379\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6882 - accuracy: 0.5593 - val_loss: 0.6902 - val_accuracy: 0.5172\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6898 - accuracy: 0.5296 - val_loss: 0.6909 - val_accuracy: 0.4138\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6899 - accuracy: 0.4259 - val_loss: 0.6896 - val_accuracy: 0.6293\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6911 - accuracy: 0.5111 - val_loss: 0.6899 - val_accuracy: 0.5345\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6917 - accuracy: 0.4593 - val_loss: 0.6898 - val_accuracy: 0.5862\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6910 - accuracy: 0.5778 - val_loss: 0.6895 - val_accuracy: 0.6121\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6893 - accuracy: 0.5074 - val_loss: 0.6900 - val_accuracy: 0.5862\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6907 - accuracy: 0.5556 - val_loss: 0.6901 - val_accuracy: 0.5948\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6885 - accuracy: 0.4963 - val_loss: 0.6897 - val_accuracy: 0.5948\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.6889 - accuracy: 0.5556 - val_loss: 0.6903 - val_accuracy: 0.4914\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.6899 - accuracy: 0.5259 - val_loss: 0.6903 - val_accuracy: 0.5431\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6889 - accuracy: 0.4889 - val_loss: 0.6895 - val_accuracy: 0.5086\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6903 - accuracy: 0.4778 - val_loss: 0.6891 - val_accuracy: 0.6379\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6916 - accuracy: 0.5074 - val_loss: 0.6895 - val_accuracy: 0.5259\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6916 - accuracy: 0.5148 - val_loss: 0.6898 - val_accuracy: 0.4310\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6885 - accuracy: 0.5185 - val_loss: 0.6892 - val_accuracy: 0.5517\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6895 - accuracy: 0.5222 - val_loss: 0.6895 - val_accuracy: 0.5431\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6894 - accuracy: 0.4963 - val_loss: 0.6896 - val_accuracy: 0.4655\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.6867 - accuracy: 0.5333 - val_loss: 0.6900 - val_accuracy: 0.3879\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6860 - accuracy: 0.4370 - val_loss: 0.6903 - val_accuracy: 0.4138\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.6887 - accuracy: 0.4741 - val_loss: 0.6893 - val_accuracy: 0.5431\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.6901 - accuracy: 0.4407 - val_loss: 0.6891 - val_accuracy: 0.5776\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6875 - accuracy: 0.5259 - val_loss: 0.6896 - val_accuracy: 0.5431\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6903 - accuracy: 0.5074 - val_loss: 0.6892 - val_accuracy: 0.5603\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6902 - accuracy: 0.4963 - val_loss: 0.6912 - val_accuracy: 0.3448\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6899 - accuracy: 0.4556 - val_loss: 0.6904 - val_accuracy: 0.4138\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.6911 - accuracy: 0.4370 - val_loss: 0.6902 - val_accuracy: 0.4569\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.6864 - accuracy: 0.5037 - val_loss: 0.6902 - val_accuracy: 0.5517\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6905 - accuracy: 0.4704 - val_loss: 0.6898 - val_accuracy: 0.4828\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6893 - accuracy: 0.5333 - val_loss: 0.6912 - val_accuracy: 0.3621\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6890 - accuracy: 0.4481 - val_loss: 0.6915 - val_accuracy: 0.3190\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6888 - accuracy: 0.4481 - val_loss: 0.6903 - val_accuracy: 0.3966\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6861 - accuracy: 0.4815 - val_loss: 0.6894 - val_accuracy: 0.5172\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6906 - accuracy: 0.4630 - val_loss: 0.6891 - val_accuracy: 0.6207\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6891 - accuracy: 0.4815 - val_loss: 0.6890 - val_accuracy: 0.5690\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6901 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.4828\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6938 - accuracy: 0.4778 - val_loss: 0.6904 - val_accuracy: 0.4138\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6901 - accuracy: 0.4963 - val_loss: 0.6903 - val_accuracy: 0.3966\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6896 - accuracy: 0.5111 - val_loss: 0.6910 - val_accuracy: 0.3793\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6887 - accuracy: 0.4111 - val_loss: 0.6897 - val_accuracy: 0.4224\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6902 - accuracy: 0.4741 - val_loss: 0.6902 - val_accuracy: 0.4052\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6914 - accuracy: 0.4704 - val_loss: 0.6899 - val_accuracy: 0.5000\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6921 - accuracy: 0.4741 - val_loss: 0.6888 - val_accuracy: 0.6466\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6910 - accuracy: 0.4778 - val_loss: 0.6894 - val_accuracy: 0.5000\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6903 - accuracy: 0.4852 - val_loss: 0.6901 - val_accuracy: 0.4828\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 0s 11ms/step - loss: 0.6890 - accuracy: 0.5333 - val_loss: 0.6891 - val_accuracy: 0.5517\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6922 - accuracy: 0.5074 - val_loss: 0.6892 - val_accuracy: 0.5517\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6939 - accuracy: 0.4741 - val_loss: 0.6886 - val_accuracy: 0.6121\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6906 - accuracy: 0.5741 - val_loss: 0.6893 - val_accuracy: 0.5172\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.6882 - accuracy: 0.5222 - val_loss: 0.6901 - val_accuracy: 0.4138\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6906 - accuracy: 0.4481 - val_loss: 0.6904 - val_accuracy: 0.3534\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6916 - accuracy: 0.4444 - val_loss: 0.6910 - val_accuracy: 0.3448\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6877 - accuracy: 0.4296 - val_loss: 0.6903 - val_accuracy: 0.3966\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 0s 9ms/step - loss: 0.6932 - accuracy: 0.5037 - val_loss: 0.6900 - val_accuracy: 0.4397\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6905 - accuracy: 0.5333 - val_loss: 0.6905 - val_accuracy: 0.3534\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 0s 10ms/step - loss: 0.6892 - accuracy: 0.4593 - val_loss: 0.6894 - val_accuracy: 0.5000\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 0s 12ms/step - loss: 0.6896 - accuracy: 0.5333 - val_loss: 0.6892 - val_accuracy: 0.5259\n",
            "90/90 [==============================] - 0s 2ms/step - loss: 0.6871 - accuracy: 0.5778\n",
            "Keras: \n",
            "accuracy: 57.78%\n",
            "predictions shape: (116, 3)\n",
            "[ True False False False False False  True False False False False False\n",
            "  True False False  True False False  True  True  True  True  True  True\n",
            "  True False False False False False  True False  True False False False\n",
            "  True False False False False False  True  True  True False False False\n",
            "  True False False False False False  True False False  True  True  True\n",
            "  True  True  True False False False False False False False False False\n",
            " False False False False  True  True  True  True  True  True False False\n",
            "  True False False  True False False False False False  True False False\n",
            "  True  True  True  True False False  True  True  True False  True  True\n",
            "  True  True  True  True  True  True False False False  True False False\n",
            "  True False False  True False False False False False  True  True  True\n",
            "  True  True  True  True False False False False False False False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True  True  True False False False False False False  True  True  True\n",
            " False  True  True False False False  True False False False False False\n",
            "  True False False False False False False False False False False False\n",
            "  True False False  True  True  True False False False  True  True  True\n",
            " False False False False False False  True  True  True False False False\n",
            " False False False False False False  True False False False False False\n",
            "  True False False  True  True  True False False False  True  True  True\n",
            " False False False  True False False  True False False False False False\n",
            "  True False False  True False False  True False False False False False\n",
            "  True False False  True False False  True False False  True False False\n",
            " False False False  True False False  True False False  True  True  True\n",
            "  True  True  True  True  True  True False False False  True False False\n",
            "  True False False False False False  True False False  True  True  True\n",
            "  True  True  True  True  True  True  True False False  True False False\n",
            " False False False  True False False  True False False False False False\n",
            "  True False False  True False False  True False False  True False False]\n",
            "[ True False  True  True  True False  True  True  True False  True False\n",
            "  True  True False False  True False False  True False  True  True  True\n",
            " False False False False  True False  True  True  True False False  True\n",
            "  True  True  True  True  True  True False  True  True False  True  True\n",
            "  True  True False  True False  True  True False  True  True  True False\n",
            "  True  True False False False False False False  True  True False False\n",
            "  True False  True False False False  True  True False  True  True  True\n",
            "  True False False  True False False  True False  True  True False False\n",
            " False  True False  True False  True  True  True  True False False False\n",
            "  True False  True False  True False  True  True False False  True False\n",
            "  True False False False False False False  True  True False False False\n",
            "  True  True False  True False False False False False  True  True  True\n",
            "  True False False  True False  True False False  True False  True False\n",
            "  True  True  True  True False False  True  True False False  True  True\n",
            "  True  True False  True  True False  True False False False  True  True\n",
            "  True  True False  True False False False  True False False False  True\n",
            " False False  True  True False  True  True  True  True False  True False\n",
            " False False False  True False False False False  True False False  True\n",
            "  True False False False  True False False False False False False False\n",
            " False  True False  True  True  True  True False False  True False  True\n",
            "  True  True False  True False  True False False  True  True  True False\n",
            "  True  True  True  True  True  True  True  True False  True False  True\n",
            " False False False  True False False  True False  True False  True  True\n",
            " False False  True False False  True False  True False  True  True False\n",
            " False False False  True  True False False  True False  True  True False\n",
            "  True False  True False  True False  True  True False False  True False\n",
            " False False False False  True  True  True False  True False  True  True\n",
            "  True False  True  True  True False False False  True  True False  True\n",
            "  True False  True False  True False False False  True False  True False]\n",
            "Keras: \n",
            "accuracy: 54.60%\n",
            "[[116  57]\n",
            " [101  74]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB8bgUxfj3fL"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJhFjqAghXvC",
        "outputId": "ed55e292-6e72-476c-cbbc-16f8f3c0a7ec"
      },
      "source": [
        "from tensorflow.keras.layers import SimpleRNN\n",
        "\n",
        "# Create the model\n",
        "# expected input data shape: (batch_size, timesteps, data_dim)\n",
        "# Dropout used to prevent over-fitting.\n",
        "# Input shape will infer the batch size by itself\n",
        "# We are using binary cross entropy even when num_class can be > 1 because this is a binary classification on an array\n",
        "# For multiclass classification, use categorical cross-entropy\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(36, return_sequences=True, input_shape=(timesteps, data_dim-2)))  # returns a sequence of vectors of dimension 40\n",
        "model.add(Dropout(drop))\n",
        "model.add(SimpleRNN(36,return_sequences=True))  # returns a sequence of vectors of dimension 40\n",
        "model.add(Dropout(drop))\n",
        "model.add(SimpleRNN(36,return_sequences=True))  # returns a sequence of vectors of dimension 40\n",
        "model.add(Dropout(drop))\n",
        "model.add(SimpleRNN(36))  # return a single vector of dimension 40\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(num_classes, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size= batchsize, epochs=1, validation_data= (X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "# Returns you the accuracy and loss\n",
        "loss, acc = model.evaluate(X_train, y_train,timesteps)\n",
        "print(\"Keras: \\n%s: %.2f%%\" % (model.metrics_names[1], acc*100))\n",
        "\n",
        "# Shape of prediction is nrow * timestep\n",
        "# Result would be that same as keras evaluate\n",
        "proba = model.predict(X_test)\n",
        "\n",
        "# proba is the probability. Here we set threshold as 0.5 to be considered true.\n",
        "print('predictions shape:', proba.shape)\n",
        "y_pred = proba > 0.5\n",
        "# Reshape to a single dimension for comparison and to create confusion matrix\n",
        "y_pred_single_dim = y_pred.reshape(proba.shape[0]*proba.shape[1])\n",
        "y_test_single_dim = y_test.reshape(y_test.shape[0]*y_test.shape[1])\n",
        "print(y_pred_single_dim)\n",
        "print(y_test_single_dim)\n",
        "matrix = confusion_matrix(y_test_single_dim, y_pred_single_dim)\n",
        "print(\"Keras: \\n%s: %.2f%%\" % (model.metrics_names[1], sum(y_pred_single_dim==y_test_single_dim)/len(y_test_single_dim)*100))\n",
        "print(matrix)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 0s 50ms/step - loss: 0.7731 - accuracy: 0.3630 - val_loss: 0.7066 - val_accuracy: 0.2845\n",
            "90/90 [==============================] - 0s 2ms/step - loss: 0.7059 - accuracy: 0.1889\n",
            "Keras: \n",
            "accuracy: 18.89%\n",
            "predictions shape: (116, 3)\n",
            "[False False False False False False False False False False False  True\n",
            " False False  True False False False False False  True False False  True\n",
            " False False False False False  True  True False False False False  True\n",
            " False False False False False  True False False  True False False  True\n",
            " False False False False False False False False False False False  True\n",
            " False False  True False False  True False False False False False  True\n",
            " False False False False False False False False  True False False False\n",
            " False False False False False  True False False  True False False  True\n",
            " False False  True False False False False False  True  True False False\n",
            " False False  True False False False False False  True False False False\n",
            " False False False False False False False False  True False False  True\n",
            " False False  True  True False False False False  True False False  True\n",
            " False False False False False False False False  True False False False\n",
            " False False  True False False  True False False  True False False  True\n",
            "  True False False False False  True False False False False False  True\n",
            " False False  True False False  True False False  True False False  True\n",
            " False False  True False False  True False False False False False  True\n",
            " False False False False False  True False False  True False False False\n",
            " False False  True False False False False False False False False  True\n",
            " False False  True False False False False False  True False False  True\n",
            " False False  True False False  True False False False False False  True\n",
            " False False False False False False False False  True False False  True\n",
            " False False False False False False False False  True False False  True\n",
            " False False  True False False False False False False False False False\n",
            " False False  True False False  True False False False False False  True\n",
            " False False False False False False False False  True False False  True\n",
            " False False False False False  True False False  True False False False\n",
            " False False False False False False False False  True False False  True\n",
            " False False  True False False False False False  True False False  True]\n",
            "[ True False  True  True  True False  True  True  True False  True False\n",
            "  True  True False False  True False False  True False  True  True  True\n",
            " False False False False  True False  True  True  True False False  True\n",
            "  True  True  True  True  True  True False  True  True False  True  True\n",
            "  True  True False  True False  True  True False  True  True  True False\n",
            "  True  True False False False False False False  True  True False False\n",
            "  True False  True False False False  True  True False  True  True  True\n",
            "  True False False  True False False  True False  True  True False False\n",
            " False  True False  True False  True  True  True  True False False False\n",
            "  True False  True False  True False  True  True False False  True False\n",
            "  True False False False False False False  True  True False False False\n",
            "  True  True False  True False False False False False  True  True  True\n",
            "  True False False  True False  True False False  True False  True False\n",
            "  True  True  True  True False False  True  True False False  True  True\n",
            "  True  True False  True  True False  True False False False  True  True\n",
            "  True  True False  True False False False  True False False False  True\n",
            " False False  True  True False  True  True  True  True False  True False\n",
            " False False False  True False False False False  True False False  True\n",
            "  True False False False  True False False False False False False False\n",
            " False  True False  True  True  True  True False False  True False  True\n",
            "  True  True False  True False  True False False  True  True  True False\n",
            "  True  True  True  True  True  True  True  True False  True False  True\n",
            " False False False  True False False  True False  True False  True  True\n",
            " False False  True False False  True False  True False  True  True False\n",
            " False False False  True  True False False  True False  True  True False\n",
            "  True False  True False  True False  True  True False False  True False\n",
            " False False False False  True  True  True False  True False  True  True\n",
            "  True False  True  True  True False False False  True  True False  True\n",
            "  True False  True False  True False False False  True False  True False]\n",
            "Keras: \n",
            "accuracy: 48.28%\n",
            "[[135  38]\n",
            " [142  33]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8rnsRdjkdwv"
      },
      "source": [
        "# GRU (simpler LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jausHe15iPKh",
        "outputId": "f48d58f3-df58-4ec7-d297-53ac6554842c"
      },
      "source": [
        "from tensorflow.keras.layers import GRU\n",
        "\n",
        "# Create the model\n",
        "# expected input data shape: (batch_size, timesteps, data_dim)\n",
        "# Dropout used to prevent over-fitting.\n",
        "# Input shape will infer the batch size by itself\n",
        "# We are using binary cross entropy even when num_class can be > 1 because this is a binary classification on an array\n",
        "# For multiclass classification, use categorical cross-entropy\n",
        "model = Sequential()\n",
        "model.add(GRU(36, return_sequences=True, input_shape=(timesteps, data_dim-2)))  # returns a sequence of vectors of dimension 40\n",
        "model.add(Dropout(drop))\n",
        "model.add(GRU(36,return_sequences=True))  # returns a sequence of vectors of dimension 40\n",
        "model.add(Dropout(drop))\n",
        "model.add(GRU(36,return_sequences=True))  # returns a sequence of vectors of dimension 40\n",
        "model.add(Dropout(drop))\n",
        "model.add(GRU(36))  # return a single vector of dimension 40\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(num_classes, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size= batchsize, epochs=1, validation_data= (X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "# Returns you the accuracy and loss\n",
        "loss, acc = model.evaluate(X_train, y_train,timesteps)\n",
        "print(\"Keras: \\n%s: %.2f%%\" % (model.metrics_names[1], acc*100))\n",
        "\n",
        "# Shape of prediction is nrow * timestep\n",
        "# Result would be that same as keras evaluate\n",
        "proba = model.predict(X_test)\n",
        "\n",
        "# proba is the probability. Here we set threshold as 0.5 to be considered true.\n",
        "print('predictions shape:', proba.shape)\n",
        "y_pred = proba > 0.5\n",
        "# Reshape to a single dimension for comparison and to create confusion matrix\n",
        "y_pred_single_dim = y_pred.reshape(proba.shape[0]*proba.shape[1])\n",
        "y_test_single_dim = y_test.reshape(y_test.shape[0]*y_test.shape[1])\n",
        "print(y_pred_single_dim)\n",
        "print(y_test_single_dim)\n",
        "matrix = confusion_matrix(y_test_single_dim, y_pred_single_dim)\n",
        "print(\"Keras: \\n%s: %.2f%%\" % (model.metrics_names[1], sum(y_pred_single_dim==y_test_single_dim)/len(y_test_single_dim)*100))\n",
        "print(matrix)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 1s 128ms/step - loss: 0.6962 - accuracy: 0.4074 - val_loss: 0.6890 - val_accuracy: 0.6724\n",
            "90/90 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.6444\n",
            "Keras: \n",
            "accuracy: 64.44%\n",
            "predictions shape: (116, 3)\n",
            "[ True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False\n",
            "  True False False  True False False  True False False  True False False]\n",
            "[ True False  True  True  True False  True  True  True False  True False\n",
            "  True  True False False  True False False  True False  True  True  True\n",
            " False False False False  True False  True  True  True False False  True\n",
            "  True  True  True  True  True  True False  True  True False  True  True\n",
            "  True  True False  True False  True  True False  True  True  True False\n",
            "  True  True False False False False False False  True  True False False\n",
            "  True False  True False False False  True  True False  True  True  True\n",
            "  True False False  True False False  True False  True  True False False\n",
            " False  True False  True False  True  True  True  True False False False\n",
            "  True False  True False  True False  True  True False False  True False\n",
            "  True False False False False False False  True  True False False False\n",
            "  True  True False  True False False False False False  True  True  True\n",
            "  True False False  True False  True False False  True False  True False\n",
            "  True  True  True  True False False  True  True False False  True  True\n",
            "  True  True False  True  True False  True False False False  True  True\n",
            "  True  True False  True False False False  True False False False  True\n",
            " False False  True  True False  True  True  True  True False  True False\n",
            " False False False  True False False False False  True False False  True\n",
            "  True False False False  True False False False False False False False\n",
            " False  True False  True  True  True  True False False  True False  True\n",
            "  True  True False  True False  True False False  True  True  True False\n",
            "  True  True  True  True  True  True  True  True False  True False  True\n",
            " False False False  True False False  True False  True False  True  True\n",
            " False False  True False False  True False  True False  True  True False\n",
            " False False False  True  True False False  True False  True  True False\n",
            "  True False  True False  True False  True  True False False  True False\n",
            " False False False False  True  True  True False  True False  True  True\n",
            "  True False  True  True  True False False False  True  True False  True\n",
            "  True False  True False  True False False False  True False  True False]\n",
            "Keras: \n",
            "accuracy: 53.74%\n",
            "[[122  51]\n",
            " [110  65]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kNWhcxiimil"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wday4--iip5t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}